{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YMYIUMT_hxDi"
   },
   "source": [
    "# **Web Scraping for ZipRecruiter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VmjpvFsciGpe"
   },
   "source": [
    "## **Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "bN7aQ0ypKiZF",
    "outputId": "708b8519-71d1-48fd-b1ce-fb879174e42c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\tyler\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from xgboost) (1.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\tyler\\anaconda3\\lib\\site-packages (from xgboost) (1.16.5)\n"
     ]
    }
   ],
   "source": [
    "#!pip install bs4\n",
    "#!pip install numpy\n",
    "#!pip install pandas\n",
    "\n",
    "#!pip install ast\n",
    "#!pip install matplotlib\n",
    "#!pip install nltk\n",
    "#!pip install wordcloud\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hc-Lwxw1PRyG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tyler\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tyler\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import nltk\n",
    "import os\n",
    "import csv\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "#from hcluster import pdist, linkage, dendrogram\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nQ7-7dM2ivo_"
   },
   "source": [
    "## Scraping Zip Recruiter for skills in data science job postings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Sraping Zip Recruiter for Job postings to find relevant skills required. Much difficulty scraping this website as it uses an \"infinite scroll\" system to load more results. Investigated the use of Selenium which can act as a human and click buttons on the page, but browser drivers were needed so considered this not an acceptable option for this project. In the end I decided to make a loop scrolling through 5 pages of each city (20 results per page) to gain the data as typically only 5 pages were available per city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Scrapping for 100 data points (20 points per page = 5 pages)\n",
    "# #create an empty DF and set pages = 6\n",
    "# zip_table = pd.DataFrame()\n",
    "# URL = 'https://www.ziprecruiter.com/Jobs/Data-Scientist/--in-Toronto,ON'\n",
    "# #i = 1\n",
    "# for i in range(1,20):\n",
    "#     time.sleep(0.5)\n",
    "# #conducting a request of the stated URL above:\n",
    "#     if i == 1:\n",
    "#         page = requests.get(URL)\n",
    "#         #specifying a desired format of “page” using the html parser - this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "#         soup = BeautifulSoup(page.text, 'html.parser')\n",
    "#     elif i > 1:\n",
    "#         URL = 'https://www.ziprecruiter.com/Jobs/Data-Scientist/--in-Toronto,ON/'\n",
    "#         URL+=str(i)\n",
    "#         page = requests.get(URL)\n",
    "#         #specifying a desired format of “page” using the html parser - this allows python to read the various components of the page, rather than treating it as one long string.\n",
    "#         soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "#     for jobs in soup.find_all(\"article\", class_ = \"job_result\"): \n",
    "#       try:\n",
    "#           job_title = jobs.find('span', class_ = 'just_job_title').text.strip()\n",
    "#       #print(job_title)\n",
    "#       except:\n",
    "#           job_title = None\n",
    "\n",
    "#       try:\n",
    "#         location = jobs.find('span', class_ = 'location').text.strip()\n",
    "#       except:\n",
    "#         location = None\n",
    "\n",
    "#       try:\n",
    "#         company = jobs.find('span', class_ = 'name').text.strip()\n",
    "#       except:\n",
    "#         company = None\n",
    "\n",
    "#       try:\n",
    "#         link = jobs.find('p', attrs={'class': 'job_snippet'}).find('a', href=True)['href']\n",
    "#       except:\n",
    "#         link = None\n",
    "\n",
    "#       try:\n",
    "#         salary = jobs.find('span', attrs={'class': 'data_item'}).text.strip()\n",
    "#       except:\n",
    "#         salary = None    \n",
    "    \n",
    "#       zip_table = zip_table.append({\n",
    "#           'job_title': job_title, \n",
    "#           'company': company,\n",
    "#           'location': location,\n",
    "#           'salary': salary,\n",
    "#           'link': link}, ignore_index = True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, a list of skills was made using terms found in the kaggle data science survey. Previously, links were obained by scraping Zip Recuiter as the job posting descriptions reside in a seprate URL. The new URLs are scraped below, and the regular expressions library is utilyzed to determine if the skills are mentioned in the job postings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# skills = 'deep learning', 'machine learning', 'ai' ,'artificial intelligence', 'ml','data science', 'cost analysis','prediction','code','coding','data architecture','programming','machine learning','ai','statistics','mathematic','communication','critical thinking','visualization','master','professional','bachelor','business decisions','operationalizing','infrastructure','prototype','workflows','modeling','statistical','cloud','jupyter','rstudio','pycharm','atom','matlab','visual studio','spyder','vim','notepad','sublime','google colab','google','google cloud','notebook','ai platform','datalab','binder','ibm watson','ibm watson studio','ocean','python','r','sql','c','c++','java','javascript','typescript','bash','ggplot','matplotlib','plotly','d3','shiny','seaborn','geoplotlib','leaflet','cpu','gpu','tpu','linear','logistic','regression','forest','gradient','bayesian','neural networks','neural','dense','convolutional','transformer','ml','augmentation','automated','feature engineering','selection','architecture','','pipelines','image','video','pil','cv2','skimage','u-net','mask r-cnn','image segmentation','object detection  ','yolov3','retinanet','image classification','vgg','inception','resnet','gan','vae','computer vision','generative networks','word embeddings','vectors','glove','fasttext','word2ve','encoder','decorder seq2seq','vanilla','contextualized embeddings','elmo','cove','gpt-2','bert','xlnet','scikit-learn','tensorflow','keras','randomforest','xgboost','pyTorch','caret','lightgbm','spark mlib','fast.ai','amazon','azure','salesforce','alibaba','aws'\n",
    "# #list of skills]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip_table['descript']='text'\n",
    "# for skill in skills:\n",
    "#     zip_table[skill] = np.zeros(len(zip_table))\n",
    "# #reset_index\n",
    "# zip_table = zip_table.reset_index()\n",
    "# zip_table.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized=[]\n",
    "# for i in range(len(zip_table)):\n",
    "#     detail = requests.get(zip_table.iloc[i,3]) \n",
    "#     soup = BeautifulSoup(detail.text, \"lxml\")\n",
    "    \n",
    "#     #BELOW IS OPTION 2 (CAN NOT HANDLE MULTIPLE WORD SKILLS)\n",
    "# #     try:\n",
    "# #         text = soup.find(\"div\", {'class' : 'job_details_container'}).text.strip().lower()\n",
    "# #       # Remove the html tags\n",
    "# #         re.sub(r'<[^<]+?>','', text)\n",
    "# #   # Tokenize the text\n",
    "# #     except:\n",
    "# #         text = None\n",
    "# #     zip_table.iloc[i,6] = text\n",
    "# #     tokenizer = RegexpTokenizer('\\W+',gaps=True)\n",
    "# #     text=tokenizer.tokenize(str(text))\n",
    "# #   # Extract the common skills\n",
    "# #     common_value=list(set(skills).intersection(text))\n",
    "# #     for word in common_value:\n",
    "# #         zip_table.loc[i, word]=1\n",
    "#   #get the text without none\n",
    "#     try:\n",
    "#         text = soup.find(\"div\", {'class' : 'job_details_container'}).text.strip().lower()\n",
    "#         string = re.sub(r'\\,', ' ', text) # remove \",\"\n",
    "#         #string = re.sub(r'\\n',' ',string) \n",
    "#         string = re.sub(r'\\,', ' ', string) # remove \",\"\n",
    "#         string = re.sub('/', ' ', string) # remove \"/\"\n",
    "#         string = re.sub(r'\\(', ' ', string) # remove \"(\"\n",
    "#         string = re.sub(r'\\)', ' ', string) # remove \")\"\n",
    "#         string = re.sub(' +',' ',string) # remove more than one space \n",
    "#         print(string)\n",
    "        \n",
    "#     except:\n",
    "#         text = None\n",
    "#         text = \"\" \n",
    "#     #zip_table.iloc[i,6] = string\n",
    "#     for sk in skills :\n",
    "#         if any(x in sk for x in ['+']):\n",
    "#             skk = re.escape(sk)\n",
    "#         else:\n",
    "#             skk = sk\n",
    "#         result = re.search(r'(?:^|(?<=\\s))' + skk + r'(?=\\s|$)',text)\n",
    "        \n",
    "#         if result:\n",
    "\n",
    "#             zip_table[sk][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #zip_table\n",
    "\n",
    "# zip_table = zip_table[zip_table.descript != \"none\"]\n",
    "# zip_table = zip_table.drop_duplicates(keep='first')\n",
    "# zip_table=zip_table.fillna(0)\n",
    "# #zip_table = zip_table.loc[:, (zip_table != 0).any(axis=0)]\n",
    "# #zip_table.to_csv(r'C:\\Users\\Tyler\\Documents\\Python notes\\Project\\ZipRecruiter.csv', index = False)\n",
    "# zip_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, an attempt was made to obtain the most frequently used word across descriptions to mine for skills. Even with the StopWords libary, there were to many general terms used and no skills were obtained. It is clear that a large list of terms needs to be created in order to find the skills with highest importance (frequency). This was done in the above work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>company</th>\n",
       "      <th>job_title</th>\n",
       "      <th>link</th>\n",
       "      <th>location</th>\n",
       "      <th>salary</th>\n",
       "      <th>descript</th>\n",
       "      <th>deep learning</th>\n",
       "      <th>machine learning</th>\n",
       "      <th>ai</th>\n",
       "      <th>...</th>\n",
       "      <th>pyTorch</th>\n",
       "      <th>caret</th>\n",
       "      <th>lightgbm</th>\n",
       "      <th>spark mlib</th>\n",
       "      <th>fast.ai</th>\n",
       "      <th>amazon</th>\n",
       "      <th>azure</th>\n",
       "      <th>salesforce</th>\n",
       "      <th>alibaba</th>\n",
       "      <th>aws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>rockwell automation</td>\n",
       "      <td>Software Data Architect</td>\n",
       "      <td>https://www.ziprecruiter.com/eclk/fAQRWYLiCODH...</td>\n",
       "      <td>Toronto, ON</td>\n",
       "      <td>Full-Time</td>\n",
       "      <td>text</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>PagerDuty</td>\n",
       "      <td>Senior Data Engineer - Analytics</td>\n",
       "      <td>https://www.ziprecruiter.com/eclk/PrjVFPG3P6gb...</td>\n",
       "      <td>Toronto, ON</td>\n",
       "      <td>Full-Time</td>\n",
       "      <td>text</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Goodfood Market Corp.</td>\n",
       "      <td>BI Engineer</td>\n",
       "      <td>https://www.ziprecruiter.com/c/Goodfood-Market...</td>\n",
       "      <td>Toronto, ON</td>\n",
       "      <td>Full-Time</td>\n",
       "      <td>text</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Interac</td>\n",
       "      <td>Senior Data Engineer</td>\n",
       "      <td>https://www.ziprecruiter.com/eclk/gptSw66Ty4w_...</td>\n",
       "      <td>Toronto, ON</td>\n",
       "      <td>Full-Time</td>\n",
       "      <td>text</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Project X</td>\n",
       "      <td>Consultant- Big Data Developer (4 Positions)</td>\n",
       "      <td>https://www.ziprecruiter.com/c/Project-X/Job/C...</td>\n",
       "      <td>Toronto, ON</td>\n",
       "      <td>Full-Time</td>\n",
       "      <td>text</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                company                                     job_title  \\\n",
       "0      0    rockwell automation                       Software Data Architect   \n",
       "1      1              PagerDuty              Senior Data Engineer - Analytics   \n",
       "2      2  Goodfood Market Corp.                                   BI Engineer   \n",
       "3      3                Interac                          Senior Data Engineer   \n",
       "4      4              Project X  Consultant- Big Data Developer (4 Positions)   \n",
       "\n",
       "                                                link     location     salary  \\\n",
       "0  https://www.ziprecruiter.com/eclk/fAQRWYLiCODH...  Toronto, ON  Full-Time   \n",
       "1  https://www.ziprecruiter.com/eclk/PrjVFPG3P6gb...  Toronto, ON  Full-Time   \n",
       "2  https://www.ziprecruiter.com/c/Goodfood-Market...  Toronto, ON  Full-Time   \n",
       "3  https://www.ziprecruiter.com/eclk/gptSw66Ty4w_...  Toronto, ON  Full-Time   \n",
       "4  https://www.ziprecruiter.com/c/Project-X/Job/C...  Toronto, ON  Full-Time   \n",
       "\n",
       "  descript  deep learning  machine learning   ai  ...  pyTorch  caret  \\\n",
       "0     text            0.0               0.0  0.0  ...      0.0    0.0   \n",
       "1     text            0.0               0.0  0.0  ...      0.0    0.0   \n",
       "2     text            0.0               0.0  0.0  ...      0.0    0.0   \n",
       "3     text            0.0               0.0  0.0  ...      0.0    0.0   \n",
       "4     text            0.0               0.0  0.0  ...      0.0    0.0   \n",
       "\n",
       "   lightgbm  spark mlib  fast.ai  amazon  azure  salesforce  alibaba  aws  \n",
       "0       0.0         0.0      0.0     0.0    0.0         0.0      0.0  0.0  \n",
       "1       0.0         0.0      0.0     0.0    0.0         0.0      0.0  0.0  \n",
       "2       0.0         0.0      0.0     0.0    0.0         0.0      0.0  0.0  \n",
       "3       0.0         0.0      0.0     0.0    0.0         0.0      0.0  0.0  \n",
       "4       0.0         0.0      0.0     0.0    0.0         0.0      0.0  1.0  \n",
       "\n",
       "[5 rows x 141 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#zip_table.to_csv(r'C:\\Users\\Tyler\\Documents\\Python notes\\Project\\ZipRecruiter.csv', index = False)\n",
    "zip_table = pd.read_csv(r'./ZipRecruiter.csv')\n",
    "zip_table.head()\n",
    "\n",
    "# wf = zip_table.descript.str.cat(sep=' ')\n",
    "# #function to split text into word\n",
    "# tokens = word_tokenize(wf)\n",
    "# vocabulary = set(tokens)\n",
    "# print(len(vocabulary))\n",
    "# from nltk.corpus import stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# tokens = [w for w in tokens if not w in stop_words]\n",
    "# frequency_dist = nltk.FreqDist(tokens)\n",
    "# sorted(frequency_dist,key=frequency_dist.__getitem__, reverse=True)[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the top 500 words are not relevant towards finding skills wanted in the field of data science."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MIE1624_Indeed.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
